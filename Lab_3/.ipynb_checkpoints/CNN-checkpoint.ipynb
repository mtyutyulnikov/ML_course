{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 3.1 - Сверточные нейронные сети (Convolutional Neural Networks)\n",
    "\n",
    "Это последнее задание на numpy, вы до него дожили! Остался последний марш-бросок, дальше только PyTorch.\n",
    "\n",
    "В этом задании вы реализуете свою собственную сверточную нейронную сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUPY_ACCELERATORS=cub,cutensor,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer, ConvolutionalLayer, MaxPoolingLayer, Flattener\n",
    "from model import ConvNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD, Adam\n",
    "from metrics import multiclass_accuracy\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "На этот раз мы не будем их преобразовывать в один вектор, а оставим размерности (num_samples, 32, 32, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_X = train_X.astype(np.float) / 255.0\n",
    "    test_X = test_X.astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_X, axis = 0)\n",
    "    train_X -= mean_image\n",
    "    test_X -= mean_image\n",
    "    return train_X, test_X\n",
    "    \n",
    "train_X, test_X = prepare_for_neural_network(x_train, x_test)\n",
    "# Split train into train and val\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, y_train, random_state=42, test_size=0.3)\n",
    "test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_X.reshape((-1, 28, 28, 1))\n",
    "val_X = val_X.reshape((-1, 28, 28, 1))\n",
    "test_X = test_X.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 28, 28, 1), (18000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, val_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итак, оверфитим маленький набор данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_resized = train_X[:].repeat(8, axis=1).repeat(8, axis=2)\n",
    "# val_X_resized = val_X[:].repeat(8, axis=1).repeat(8, axis=2)\n",
    "# train_X_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_resized_gpu = cp.asarray(train_X_resized)\n",
    "# val_X_resized_gpu = cp.asarray(val_X_resized)\n",
    "# train_y_gpu = cp.asarray(train_y)\n",
    "# val_y_gpu = cp.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_resized = train_X[:128].repeat(3, axis=1).repeat(3, axis=2)\n",
    "# val_X_resized = val_X[:128].repeat(3, axis=1).repeat(3, axis=2)\n",
    "# train_X_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64af58770d384908810103aa0760493e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=328.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD -------------------------------------------------------------------\n",
      "conv time: 0.00111\n",
      "relu time: 0.00031\n",
      "maxpooling time: 0.00024\n",
      "conv time: 0.00042\n",
      "relu time: 0.00019\n",
      "maxpooling time: 0.00006\n",
      "conv time: 0.00024\n",
      "relu time: 0.00006\n",
      "conv time: 0.00036\n",
      "relu time: 0.00005\n",
      "conv time: 0.00036\n",
      "relu time: 0.00005\n",
      "maxpooling time: 0.00018\n",
      "flattener time: 0.00000\n",
      "FC time: 0.00007\n",
      "relu time: 0.00005\n",
      "dropout time: 0.00026\n",
      "FC time: 0.00005\n",
      "relu time: 0.00017\n",
      "dropout time: 0.00023\n",
      "FC time: 0.00005\n",
      "BACKPROP ==================================================================\n",
      "FC time: 0.00024\n",
      "dropout time: 0.00002\n",
      "relu time: 0.00001\n",
      "FC time: 0.00011\n",
      "dropout time: 0.00014\n",
      "relu time: 0.00001\n",
      "FC time: 0.00021\n",
      "flattener time: 0.00000\n",
      "maxpooling time: 0.01549\n",
      "relu time: 0.00002\n",
      "(128, 13, 13, 3, 3, 256) (128, 13, 13, 256)\n",
      "conv time: 0.83378\n",
      "relu time: 0.00003\n",
      "(128, 13, 13, 3, 3, 384) (128, 13, 13, 256)\n",
      "conv time: 0.24035\n",
      "relu time: 0.00002\n",
      "(128, 13, 13, 3, 3, 192) (128, 13, 13, 384)\n",
      "conv time: 0.31569\n",
      "maxpooling time: 0.17432\n",
      "relu time: 0.00002\n",
      "(128, 27, 27, 5, 5, 64) (128, 27, 27, 192)\n",
      "conv time: 0.40389\n",
      "maxpooling time: 0.27255\n",
      "relu time: 0.00002\n",
      "(128, 55, 55, 11, 11, 1) (128, 55, 55, 64)\n",
      "conv time: 0.46373\n",
      "FORWARD -------------------------------------------------------------------\n",
      "conv time: 0.03604\n",
      "relu time: 0.00039\n",
      "maxpooling time: 0.00038\n",
      "conv time: 0.04157\n",
      "relu time: 0.00004\n",
      "maxpooling time: 0.00045\n",
      "conv time: 0.00029\n",
      "relu time: 0.00017\n",
      "conv time: 0.00026\n",
      "relu time: 0.00015\n",
      "conv time: 0.00026\n",
      "relu time: 0.00016\n",
      "maxpooling time: 0.00020\n",
      "flattener time: 0.00001\n",
      "FC time: 0.00009\n",
      "relu time: 0.00004\n",
      "dropout time: 0.00030\n",
      "FC time: 0.00016\n",
      "relu time: 0.00004\n",
      "dropout time: 0.00014\n",
      "FC time: 0.00015\n",
      "BACKPROP ==================================================================\n",
      "FC time: 0.00025\n",
      "dropout time: 0.00013\n",
      "relu time: 0.00013\n",
      "FC time: 0.00024\n",
      "dropout time: 0.00013\n",
      "relu time: 0.00012\n",
      "FC time: 0.00023\n",
      "flattener time: 0.00000\n",
      "maxpooling time: 0.01738\n",
      "relu time: 0.00003\n",
      "(128, 13, 13, 3, 3, 256) (128, 13, 13, 256)\n",
      "conv time: 0.72916\n",
      "relu time: 0.00002\n",
      "(128, 13, 13, 3, 3, 384) (128, 13, 13, 256)\n",
      "conv time: 0.23972\n",
      "relu time: 0.00001\n",
      "(128, 13, 13, 3, 3, 192) (128, 13, 13, 384)\n",
      "conv time: 0.31552\n",
      "maxpooling time: 0.17377\n",
      "relu time: 0.00002\n",
      "(128, 27, 27, 5, 5, 64) (128, 27, 27, 192)\n",
      "conv time: 0.40370\n",
      "maxpooling time: 0.26972\n",
      "relu time: 0.00002\n",
      "(128, 55, 55, 11, 11, 1) (128, 55, 55, 64)\n",
      "conv time: 0.46400\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 301,989,888 bytes (allocated so far: 7,639,389,184 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-99ca2c382eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ML_course/Lab_3/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML_course/Lab_3/optim.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, weights, grad, learning_rate)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mv_bias_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0ms_bias_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_bias_corr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_bias_corr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_out_args_from_optionals\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_init\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core.ndarray._init_fast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Out of memory allocating 301,989,888 bytes (allocated so far: 7,639,389,184 bytes)."
     ]
    }
   ],
   "source": [
    "data_size = 41984\n",
    "model = ConvNet(input_shape=(224,224,1), n_output_classes=10, conv1_channels=40, conv2_channels=40)\n",
    "# dataset = Dataset(train_X[:41984], train_y[:41984], val_X[:17920], val_y[:17920])\n",
    "dataset = Dataset(train_X_resized[:128], train_y[:128], val_X_resized[:128], val_y[:128])\n",
    "\n",
    "# model = ConvNet(input_shape=(28,28,1), n_output_classes=10, conv1_channels=40, conv2_channels=40)\n",
    "# dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "\n",
    "trainer = Trainer(model, dataset, Adam(), learning_rate=1e-3, num_epochs=100, batch_size=128)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "a[1 : -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
